CUDA_VISIBLE_DEVICES=1,2 WORLD_SIZE=2 torchrun --nproc_per_node=2 --master_port=4321 finetune.py \
    --base_model "meta-llama/Llama-2-7b-hf" \
    --num_epochs 3 \
    --cutoff_len 2048 \
    --data_path "data/gsm/gsm_data_1000.jsonl" \
    --output_dir "trained_lora/7b-gsm-with-13b" \
    --lora_target_modules "[q_proj,k_proj,v_proj,o_proj,up_proj,down_proj,gate_proj,embed_tokens,lm_head]" \
    --lora_r 16 \
    --micro_batch_size 8 \
    --batch_size 64 \
    --learning_rate 3e-4 \
    --val_set_size 0 \
    --use_chat_prompt \
    --train_on_inputs False \
    --seed 1 \
    --resume_from_checkpoint "extracted_lora/13b-to-7b-gsm" \
    --delta_path "extracted_lora/13b-to-7b-gsm-delta" \